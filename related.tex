\chapter{Related Work}
\label{related}

Problems in the training of deep neural networks have been encountered in several studies.
For example, \cite{xavier, he} investigated vanishing/exploding gradient propagation and gave weight initialization methods as the solution. \cite{evop} suggested that vanishing/exploding gradients might relate to the sum of the reciprocals of the hidden layer widths.
\cite{opt_prob, saddle} stated that saddle points are more likely than local minima to be a problem for training deep neural networks.
\cite{degrade1, degrade2, resnet1} exposed the \textit{degradation} problem: the performance of a deep neural network degrades as the depth increases.

The correlation between the nodes of hidden layers within a deep neural network is the main focus of this paper, and several kinds of correlations have been discussed in the literature.
%, while several kinds of correlations have been discussed. In this work, we proposed a different problem related to the correlation between two nodes in a hidden layer.
\cite{mft:info} surveyed the propagation of the correlation between two different inputs after several layers.
\cite{whiten1, whiten2} suggested that the input features must be whitened (i.e., zero-mean, unit variances and uncorrelated) to achieve a faster training speed.

Dynamical isometry is one of the conditions that make ultra-deep network training more feasible.
\cite{mft:linear} reported dynamical isometry to theoretically ensure depth-independent learning speed.
\cite{mft:sigmoid, mft:spectral} suggested several ways to achieve dynamical isometry for various settings of network architecture, and \cite{mft:cnn, mft:rnn} practically trained ultra-deep networks in various tasks.
